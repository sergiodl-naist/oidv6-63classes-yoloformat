{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a027c73",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sergiodl-naist/yolov4-training-with-oidv6/blob/main/YOLOv4_OIDv6_Training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda0e69",
   "metadata": {},
   "source": [
    "# Training YOLOv4 with Custom Dataset from Open Images Database v6 (OIDv6)\n",
    "\n",
    "This Jupyter Notebook is based on [YOLOv4: A step-by-step guide for Custom Data Preparation with Code](https://techylem.com/yolov4-guide-with-code/) with information of [TRAIN A CUSTOM YOLOv4 OBJECT DETECTOR (Using Google Colab)](https://medium.com/analytics-vidhya/train-a-custom-yolov4-object-detector-using-google-colab-61a659d4868).\n",
    "\n",
    "It also uses the [OIDv6 tool](https://github.com/DmitryRyumin/OIDv6) to download the dataset from Google's [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html). And its helper scripts are inspired from [OIDv4_ToolKit](https://github.com/ahsan44411/OIDv4_ToolKit)\n",
    "\n",
    "Details about darknet customization options to make them work better on Google's Colab are available at\n",
    "\n",
    "* [Darknet FAQ](https://www.ccoderun.ca/programming/darknet_faq/)\n",
    "* [CFG Parameters in the [net] section](https://github.com/AlexeyAB/darknet/wiki/CFG-Parameters-in-the-%5Bnet%5D-section)\n",
    "\n",
    "Information about Yolov4-tiny training and when to stop training at [AlexeyAB/darknet](https://github.com/AlexeyAB/darknet#how-to-train-tiny-yolo-to-detect-your-custom-objects).\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* CUDA Runtime and Driver API\n",
    "  - Installation Instructions https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html\n",
    "    Prefer Packager Manager Installation and follow the instructions for adding their Repository https://developer.nvidia.com/cuda-downloads\n",
    "  - Follow Post-Installation instructions and add cuda to your path. Also install and compile and verify your installation with the Sample Code.\n",
    "\n",
    "* cuDNN normal and dev installation\n",
    "  - Installation Instructions https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html\n",
    "    It uses the same repository than the CUDA Runtime/API Driver for Package Manager Installation\n",
    "  - Compile and very your installation with the Sample Code\n",
    "\n",
    "* OpenCV normal and dev installation\n",
    "  - Compile from source and install. \n",
    "    There is many tutorials, including those in OpenCV website https://docs.opencv.org/4.5.3/d7/d9f/tutorial_linux_install.html. I found this one, easy to follow https://www.itsfoss.net/how-to-install-and-configure-opencv-on-ubuntu-20-04/. Be sure to follow current tutorials.\n",
    "    Though you might not need it, \"itsfoss\" tutorial does not explain about compiling and installing opencv-contrib. If you want to install it, you will need to create a build directory inside it and build it as per README.txt instructions. If you have followed the tutorial directory naming instructions you could use `cmake -D CMAKE_INSTALL_PREFIX=/usr/local -DOPENCV_EXTRA_MODULES_PATH=~/opencv_build/opencv_contrib/modules ~/opencv_build/opencv`, `make` and then `sudo make install`.\n",
    "\n",
    "## Choose an environment\n",
    "\n",
    "Please choose the type of the environment this Notebook is being run, whether is run off-line locally or in Google's Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17729610",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_COLAB_ENV = True\n",
    "BACKUP_DIR = \"Training/Backup\" # Make sure that your backup Directory exists\n",
    "MODEL_TO_TRAIN = \"yolov4-tiny\" # (Only supported options: yolov4 or yolov4-tiny)\n",
    "\n",
    "G_DRIVE_MOUNTPOINT = \"/drive\"\n",
    "G_DRIVE_ROOT = G_DRIVE_MOUNTPOINT + \"/MyDrive\"\n",
    "G_DRIVE_DATASETZIP = G_DRIVE_ROOT + \"/Training/Data/dataset.zip\"\n",
    "\n",
    "from os import path, getcwd\n",
    "if GOOGLE_COLAB_ENV:\n",
    "    CONTENT = \"/content\"\n",
    "    DATASET = CONTENT + \"/multidata\"\n",
    "    SCRIPTS = CONTENT + \"/yolov4-training-with-oidv6\"\n",
    "    DARKNET = CONTENT + \"/darknet\"\n",
    "    BACKUP_DIR = G_DRIVE_ROOT + \"/\" + BACKUP_DIR\n",
    "else:\n",
    "    CONTENT = path.realpath(getcwd())\n",
    "    DATASET = CONTENT + \"/multidata\"\n",
    "    SCRIPTS = CONTENT\n",
    "    DARKNET = CONTENT + \"/../darknet\"\n",
    "    BACKUP_DIR = CONTENT + \"/\" + BACKUP_DIR\n",
    "\n",
    "CFG_FILE = \"\"\n",
    "PRE_TRAINED_WEIGHTS = \"\"\n",
    "PTW_FILENAME = \"\"\n",
    "CUSTOM_CFG_FILE = SCRIPTS + \"/my-\" + MODEL_TO_TRAIN + \".cfg\"\n",
    "\n",
    "if MODEL_TO_TRAIN == \"yolov4\":\n",
    "    CFG_FILE = DARKNET + \"/cfg/yolov4-custom.cfg\"\n",
    "    PRE_TRAINED_WEIGHTS_URL = \"https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137\"\n",
    "    PTW_FILENAME = \"yolov4.conv.137\"\n",
    "elif MODEL_TO_TRAIN == \"yolov4-tiny\":\n",
    "    CFG_FILE = DARKNET + \"/cfg/yolov4-tiny-custom.cfg\"\n",
    "    PRE_TRAINED_WEIGHTS_URL = \"https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29\"\n",
    "    PTW_FILENAME = \"yolov4-tiny.conv.29\"\n",
    "\n",
    "# You may edit this to point to your last weights to resume training\n",
    "# PRE_TRAINED_WEIGHTS = \"$BACKUP_DIR/my-yolov4-tiny_last.weights\"\n",
    "PRE_TRAINED_WEIGHTS = DARKNET + \"/\" + PTW_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84855be",
   "metadata": {},
   "source": [
    "## Clone this repository tools\n",
    "\n",
    "On this repository I created a set of tools to easily customize darkent's YOLOv4 configuration, please clone it first if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be145f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You only need to clone the repository if you are in Google Colab\n",
    "!git clone --depth 1 https://github.com/sergiodl-naist/yolov4-training-with-oidv6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0944d624",
   "metadata": {},
   "source": [
    "The scripts will be available at `/content/yolov4-training-with-oidv6` if this notebook is run in Google's Colab.\n",
    "\n",
    "## Prepare Custom Dataset\n",
    "\n",
    "### Download your dataset\n",
    "\n",
    "**This part should be done on your local development machine.  \n",
    "If you want to use the dataset on Google's Colab, later we will upload it to Google's Drive and then it will be available there by mounting and copying the data**\n",
    "\n",
    "If you want to globally install in your session oidv6 run\n",
    "\n",
    "`$ pip3 install --user oidv6`\n",
    "\n",
    "But **it is advised to better use python environments** through `pipenv`. This will isolate the packages installed to this directory.\n",
    "\n",
    "If using `pipenv` you only need to run `$ pipenv install -r path/to/requirements.txt` to install all python dependencies. Jupyter Notebooks for local sessions will be automatically installed too. \n",
    "\n",
    "Create a `classes.txt` file where each line will be a class you want to download.\n",
    "\n",
    "You can download the classes names from https://storage.googleapis.com/openimages/web/download.html under section \"Annotations and metadata\"; row \"Metadata\" and pressing the button \"Class Names\".\n",
    "\n",
    "Think about how many pictures for training, validation and testing you want.\n",
    "Example: train - 300, validation - 75, test - 10.\n",
    "\n",
    "Where 300 images for training are the 80\\% and 75 images for validation 75 are 20\\% of a dataset of 375 per class.\n",
    "\n",
    "Note that the amount of images per class available on the OID is not the same. Some classes have under 100 images or less.\n",
    "\n",
    "Download your dataset for training with\n",
    "\n",
    "`$ oidv6 downloader en --type_data train --classes ./classes.txt --limit 300 --multi_classes`\n",
    "\n",
    "the dataset for validation with\n",
    "\n",
    "`$ oidv6 downloader en --type_data validation --classes ./classes.txt --limit 75 --multi_classes`\n",
    "\n",
    "and the dataset for testing with\n",
    "\n",
    "`$ oidv6 downloader en --type_data test --classes ./classes.txt --limit 10 --multi_classes`\n",
    "\n",
    "Note that with `--limit NN` you specify how many images you want for each class of the dataset\n",
    "\n",
    "`--multi_classes` will put all pictures on one directory (\"train\", \"validation\", \"test\" according to your `--type_data` selected) and all annotations labels inside one directory named `labels` inside your dataset directory. Without this option, each class' pictures will be downloaded into individual directories in which each one of them will have an individual `labels` directory with all annotations label files.\n",
    "\n",
    "**Some classes might not have any picture in the OID** a red warning will be shown during the download to let you know about these classes without pictures.\n",
    "\n",
    "\n",
    "### Adapt your Dataset to YOLOv4 format\n",
    "\n",
    "The annotation files for bounding boxes have this format\n",
    "\n",
    "`label_name x1 y1 x2 y2`\n",
    "\n",
    "Which is not the annotation label format compatible with YOLO which is\n",
    "\n",
    "`label_index box_center_x box_center_y box_width box_heigth`\n",
    "\n",
    "Where\n",
    "\n",
    "**label_index**: is the index of the label inside `classes.txt`\n",
    "\n",
    "**box_center_x**: is the x value* of the center of the bouding box\n",
    "\n",
    "**box_center_y**: is the y value* of the center of the bouding box\n",
    "\n",
    "**box_width**: is the width* of the bouding box\n",
    "\n",
    "**box_heigth**: is the height* of the bouding box\n",
    "\n",
    "And that such coordinates, width and height are represented by a float number between \\[0, 1\\] where 0 is the origin and 1 is the max. width or max. height.\n",
    "\n",
    "Also, the darknet tool will ask for a list of all the file paths of the images for training and validation.\n",
    "\n",
    "All this is solved with the script `prepare_dataset.py`.\n",
    "\n",
    "It will also generate the configuration file `objects.txt` needed for darknet.\n",
    "\n",
    "Run `$ python3 prepare_dataset.py /path/to/your/classes.txt`\n",
    "\n",
    "Now, inside the OIDv6 directory you will have `multidata` directory with your dataset, a copy of `classes.txt` and the configuration file `objects.txt`. \n",
    "\n",
    "Zip the 3 of them up into a `dataset.zip` file, and **upload it to your Google Drive** inside a directory structure like this: **`Training/Data/dataset.zip`**\n",
    "\n",
    "### Improving your Annotations\n",
    "\n",
    "Small objects might become hard to identify after the image is down-scaled to the Network Input Size (_width_ and _height_ parameters in \\*.cfg file). With the script `check_small_boxes.py`, which accepts as parameter an Network Input Size (e.g.: 416, 512, 608, etc), you can parse your labels files and identify those bounding boxes that will become less of 16 pixels in size after down-scalign. Such small annotations may be too small and will affect your training performance.\n",
    "  - https://github.com/AlexeyAB/darknet#how-to-improve-object-detection\n",
    "\n",
    "You may try increase your Network Input Size (if you get memory errors you might increase the parameter _subdivisions_ in your \\*.cfg file). Or/and you might try to segment your high resolution images into smaller Network Input Sized images, and your original data won't be down-scaled at all. You might try using a sliding window for segmentation with a stride smaller than the segment's width/height, so each segment have some overlapping information and objects can completely be captured.\n",
    "  - https://github.com/pjreddie/darknet/issues/1535\n",
    "  \n",
    "If you change your Network Input Size you should [recalculate your anchor boxes](https://github.com/AlexeyAB/darknet/issues/7856#issuecomment-886209744) ([Anchor Boxes — The key to quality object detection](https://towardsdatascience.com/anchor-boxes-the-key-to-quality-object-detection-ddf9d612d4f9)). There's a command to recalculate such anchor boxes according to your dataset and Network Input Size. Change it according to your needs. \n",
    "\n",
    "  `./darknet detector calc_anchors data/obj.data -num_of_clusters 9 -width 416 -height 416`\n",
    "  * `-num_of_clusters` refers to the number of pairs of coordinates of `anchors` in your yolo.cfg file.\n",
    "  * You can use the flag `-show` to see the proposed anchors. You can run the command several times and you should choose those anchors with the highest IoU.\n",
    "  * You might want to increase the num_of_clusters to be able to cover evenly and the most points shown with the `-show` flag.\n",
    "  - https://github.com/AlexeyAB/darknet#how-to-improve-object-detection\n",
    "  - Explanation and several tips: https://github.com/AlexeyAB/darknet/issues/7856\n",
    "\n",
    "Tools like DarkMark (https://www.ccoderun.ca/darkmark/Summary.html) can help you labeling your dataset, augment it and recalculating anchor boxes (and others optimizations). DarkMark uses MarkHelp's API that itself uses darknet module, so to build it you will need to build both MarkHelp and Darknet (with LIBSO=1 set in darknet's Makefile).\n",
    "\n",
    "\n",
    "### Mount Google Drive\n",
    "\n",
    "Let's mount your Google Drive into Colab's runtime (or unzip your data in the current directory if you are running this notebook off-line locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Colab's\n",
    "from google.colab import drive\n",
    "drive.mount(G_DRIVE_MOUNTPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ce8d4",
   "metadata": {},
   "source": [
    "Unzip your dataset into Colab's environment filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03bc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip \"$G_DRIVE_DATASETZIP\" -d \"$CONTENT\"\n",
    "print(\"Dataset unzipped into \" + CONTENT)\n",
    "\n",
    "!mkdir -p \"$BACKUP_DIR\"\n",
    "print(\"Backup Directory created at \" + BACKUP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c605c",
   "metadata": {},
   "source": [
    "## Prepare Darknet Framework\n",
    "\n",
    "### Download darknet source code\n",
    "\n",
    "Clone darknet project. This is a framework/tool to train and customize several YOLO versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaaefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --depth 1 https://github.com/AlexeyAB/darknet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9b425",
   "metadata": {},
   "source": [
    "Modify makefile to work with the GPU and OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd darknet\n",
    "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
    "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
    "!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n",
    "\n",
    "# Uncomment the next line if you want to build later MarkHelp/DarkMark to help you with annotations\n",
    "#!sed -i 's/LIBSO=0/LIBSO=1/' Makefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2865418",
   "metadata": {},
   "source": [
    "Check if CUDA Compiler is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df58c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/cuda/bin/nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e10b6",
   "metadata": {},
   "source": [
    "Compile darknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db183de9",
   "metadata": {},
   "source": [
    "### (Optional) Test darknet\n",
    "\n",
    "Download pre-trained YOLOv4 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c32cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a5d43",
   "metadata": {},
   "source": [
    "Defining function to show output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def imShow(path):\n",
    "  image = cv2.imread(path)\n",
    "  height, width = image.shape[:2]\n",
    "  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "  fig = plt.gcf()\n",
    "  fig.set_size_inches(18, 10)\n",
    "  \n",
    "  plt.axis(\"off\")\n",
    "  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e788d5c",
   "metadata": {},
   "source": [
    "Run the Object Recognition Model on test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964888bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/person.jpg\n",
    "imShow('predictions.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d381de0",
   "metadata": {},
   "source": [
    "### Customize Darknet's Configuration\n",
    "\n",
    "Customize `darknet/cfg/yolov4-tiny-custom.cfg` or `darknet/cfg/yolov4-custom.cfg` into your liking and name it `my-yolov4-tiny.cfg` or `my-yolov4.cfg` respectively.\n",
    "\n",
    "Or you can run the script `customize_yolov4.py` to automatically adapt them to your current dataset according to your `classes.txt` file.\n",
    "\n",
    "Changes that this script it will make:\n",
    "\n",
    "```\n",
    "[net]\n",
    "max_batches = (# of Classes * 2000)\n",
    "steps = (80% of max_batches), (90% of max_batches)\n",
    "\n",
    "#### Last section of the configuration file\n",
    "###### Three pairs if yolov4-custom, two pairs if yolov4-tiny-custom\n",
    "\n",
    "[convolutional]\n",
    "filters = ( (# of Classes + 5) * 3 )\n",
    "[yolo]\n",
    "classes = # of Classes\n",
    "\n",
    "[convolutional]\n",
    "filters = ( (# of Classes + 5) * 3 )\n",
    "[yolo]\n",
    "classes = # of Classes\n",
    "\n",
    "[convolutional]\n",
    "filters = ( (# of Classes + 5) * 3 )\n",
    "[yolo]\n",
    "classes = # of Classes\n",
    "```\n",
    "\n",
    "Run the customizing tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f884c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 \"$SCRIPTS\"/customize_yolov4.py \"$CFG_FILE\" \"$SCRIPTS\"/classes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe018d",
   "metadata": {},
   "source": [
    "You may need absolute paths in your configuration files and train/validation/test file lists. Edit them with the next command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e61e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set current valid absolute paths for the dataset information\n",
    "escaped_content = (CONTENT + \"/\").replace(\"/\", \"\\/\")\n",
    "escaped_bdir = BACKUP_DIR.replace(\"/\", \"\\/\")\n",
    "\n",
    "!sed -i \"s/train=/train=$escaped_content/\" \"$CONTENT\"/objects.txt\n",
    "!sed -i \"s/valid=/valid=$escaped_content/\" \"$CONTENT\"/objects.txt\n",
    "!sed -i \"s/names=/names=$escaped_content/\" \"$CONTENT\"/objects.txt\n",
    "!sed -i \"s/backup=\\.\\//backup=$escaped_bdir/\" \"$CONTENT\"/objects.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e82850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to absolute paths the train/validation/test file lists\n",
    "escaped_dataset = DATASET.replace(\"/\", \"\\/\")\n",
    "\n",
    "!sed -i \"s/^train/$escaped_dataset\\/train/g\" \"$DATASET\"/train.txt\n",
    "!sed -i \"s/^validation/$escaped_dataset\\/validation/g\" \"$DATASET\"/validation.txt\n",
    "!sed -i \"s/^test/$escaped_dataset\\/test/g\" \"$DATASET\"/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29eed1",
   "metadata": {},
   "source": [
    "Download pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"$PRE_TRAINED_WEIGHTS_URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5ce19",
   "metadata": {},
   "source": [
    "## Begin training\n",
    "\n",
    "If you want to use your command line and work on a head-less workstation, run and copy the next command.\n",
    "You will be able to check your results at http://localhost:8090 or http://(your-private-ip):8090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"./darknet detector train {CONTENT}/objects.txt {CUSTOM_CFG_FILE} {PRE_TRAINED_WEIGHTS} -dont-show -mjpeg_port 8090 -map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!./darknet detector train \\\n",
    "  \"$CONTENT\"/objects.txt \\\n",
    "  \"$CUSTOM_CFG_FILE\" \\\n",
    "  \"$PRE_TRAINED_WEIGHTS\" \\\n",
    "  -dont_show \\\n",
    "  -map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3d57d",
   "metadata": {},
   "source": [
    "## Test your models\n",
    "\n",
    "Customize to match your paths, create your test image sets and loop through them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378742c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path, chdir, getcwd, listdir\n",
    "\n",
    "model_to_train = '211008.yolov4-csp-o'\n",
    "\n",
    "chdir(CONTENT) # Reset reference\n",
    "base = path.realpath(path.join('..', 'TrainedModels'))\n",
    "test_images_path = path.join(base, 'Test')\n",
    "\n",
    "models = {\n",
    "    '211008.yolov4-csp-o': {\n",
    "        'Path': path.join(base, '211008.yolov4-csp-custom.7c.o'),\n",
    "        'Data': path.join(base, '211008.yolov4-csp-custom.7c.o', 'yolov4-csp-custom.data'),\n",
    "        'Cfg': path.join(base, '211008.yolov4-csp-custom.7c.o', 'yolov4-csp-custom.cfg'),\n",
    "        'Weigths': path.join(base, '211008.yolov4-csp-custom.7c.o', 'yolov4-csp-custom_best.weights'),\n",
    "        'Output': path.join(base, '211008.yolov4-csp-custom.7c.o', 'predictions'),\n",
    "    },\n",
    "}\n",
    "\n",
    "model = models[model_to_train]\n",
    "obj_data = model['Data']\n",
    "model_cfg = model['Cfg']\n",
    "model_weights = model['Weigths']\n",
    "\n",
    "chdir(model['Path'])\n",
    "\n",
    "for test_img in listdir(test_images_path):\n",
    "    img = path.join(test_images_path, test_img)\n",
    "    !\"$DARKNET\"/darknet detector -dont_show test \\\n",
    "        \"$obj_data\" \"$model_cfg\" \"$model_weights\" \\\n",
    "        \"$img\"\n",
    "    destination = path.join(model['Output'], path.basename(test_img))\n",
    "    print(destination)\n",
    "    !mv 'predictions.jpg' \"$destination\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
